\documentclass[journal]{IEEEtran}

\usepackage{palatino,epsfig,latexsym,cite,graphicx,amsmath,amssymb,amsfonts,multirow,booktabs,color,soul}
\usepackage{algorithmic,url}
\usepackage{float}
\usepackage{threeparttable}
\usepackage{amsmath}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\def\MR2{\multirow{2}[2]{*}}
\def\Hei{\hl}
\definecolor{hl}{rgb}{0.75,0.75,0.75}
\newcommand{\XG}[1]{\textcolor[rgb]{0.00,0.00,1.00}{#1}}
\newcommand{\TODO}[1]{\textcolor[rgb]{1.00,0.40,0.22}{#1}}
\sethlcolor{hl}
\usepackage{makecell}
\usepackage[caption=false,font=footnotesize]{subfig}
\renewcommand{\algorithmicrequire}{ \textbf{Input:}}      %Use Input in the format of Algorithm
\renewcommand{\algorithmicensure}{ \textbf{Output:}}     %Use Output in the format of Algorithm
\DeclareMathOperator*{\argmin}{argmin}

%\setlength{\abovecaptionskip}{-5pt}
\setlength{\floatsep}{12pt  minus 4pt}
\setlength{\textfloatsep}{13pt minus 4pt}
\renewcommand{\textfraction}{0.1}
\renewcommand{\topfraction}{0.9}
\setlength\arraycolsep{2pt}

\begin{document}

\title{{Adaptation Operator Selection with Reinforcement Learning for Multi-objective Evolutionary}
  \thanks{Thanks}}
\author{
  authors,
}% <-this % stops a space

% \markboth{IEEE Transactions on Cybernetics,~Vol.~, No.~, month~year}
% {Tian \MakeLowercase{\textit{et al.}}: SOLVING LMOPS WITH SPARSE OPTIMAL SOLUTIONS VIA UNSUPERVISED NNS}

\IEEEpubid{0000--0000/00\$00.00~\copyright~0000 IEEE}

\maketitle

\begin{abstract}
  Evolutionary Algorithms (EAs) are stochastic optimization algorithms which are greatly influenced by operators, different reproduction operators have various search range.
  Adaptive operator selection (AOS) provides the on-line control of the evolutionary algorithms that which operator should be chosen for the current individual.
  This paper proposes an AOS strategy based on Double Q-learning (DDQN), a deep Reinforcement Learning (RL) algorithm in discrete action spaces, to detect the operator should be applied.
  The two main components of AOS, Credit Assignment and Operator Selection, correspond to Reward and Action of Markov Decision Process (MDP) and the State of MDP can be represented by the features of the current individual. According to the above rules, the operator selection problem is modeled as MDP and can be solved by reinforcement learning. In the evolutionary process, RL will select an operator for each individual and update the behavioral strategy according to the effect of this selection. Experimental results on \TODO{supplement experimental instructions}.
\end{abstract}

\begin{IEEEkeywords}
  Multi-objective optimization, Reinforcement Learning, Credit assignment
\end{IEEEkeywords}

\section{Introduction}
%%% 介绍下进化多目标优化
% 由于多个目标间通常互相冲突，没有一个全局最优解，一般会有一个PF
\IEEEPARstart{M}{ultl-object} optimization problems (MOPs) are characterized by multiple objectives that often conflict with each other\cite{zhang2014efficient}. In general, MOPs have no single optimal solution on all objectives; instead, a number of solutions are optimal for different objectives, known as Pareto-optimal solutions.
% All the Pareto optimal solutions constitute the Pareto set and the set of all Pareto optimal objective vectors is the Pareto front \cite{deb2001multi}.

%%% 介绍进化算法
% 进化算法可以解决很多多目标优化问题，最近几十年有了很大的发展，
Multi-object evolutionary algorithms have attracted great attention over the past few decades and achieved outstanding performance in solving various kinds of MOPs \cite{fialho2010adaptive},
% 人们提出了很多新的算法，主要有3类，基于支配关系的，基于分解的和基于指标的算法
a number of algorithms have been proposed, e.g., indicator-based evolutionary algorithm (IBEA) \cite{IBEA}, the multi-objective evolutionary algorithm based on decomposition (MOEA/D) \cite{moead} and fast non-dominated sorting genetic algorithm (NSGA-II) \cite{nsga2}.
% 生成算子对进化算法的性能有很大的影响，有的算子注重探索能力，我们称为全局算子，有的注重局部的收敛，我们称为局部算子,那么如何平衡算法的探索和利用就成了人们研究的对象
In these algorithms, the reproduction operators have a great influence on the performance.
Generally speaking, different operators have different applications, some operators tend to explore unknown regions to find global optimal solutions, called global operators; and the others tend to converge quickly with population information, called local operators.
% 所以我们要平衡探索和利用算子
In order to accelerate the process of evolution, balanced exploration and exploitation became an important topic.

%%% 介绍AOS
% AOS就是为个体推荐合适的算子，在线控制
Adaptive operator selection (AOS) applies appropriate operators for individuals by online control while solving the problem, based on the recent performance of operators \cite{li2011multi}.
% 包含credit assignment和operator selection 两部分，
An AOS algorithm usually consists of \textit{credit assignment} and \textit{operator selection} \cite{li2013evolving}, credit assignment is to decide how much credit should be assigned to an operator based on its recent performance in the search process, operator selection is to select an operator to use at the next time point based on these current reward values \cite{frrmab}.
% 和credit assignment 尽量准确评价算子近期的效果不同，operator selection 需要平衡探索和利用，即既需要选择尽量表现好的算子来提高算法效果，又需要给表现不好的算子一些机会，因为它们在之后有可能变好。
Different from credit assignment, which only needs to evaluate operator performance as accurately as possible, operator selection should balance exploration and exploitation: one hopes to give more chances to the best operators found so far in order to maximize the cumulative improvement (exploitation), but also needs to explore poor operators in the future search (exploration).
% 通常有MAB，UCB，可以和MOEAD结合应用到多目标问题上。
% 但是由于环境是不稳定的，一开始好的算子不一定在最后好，所以就有动态MAB或者动态汤普森采样等。
% 写到related work里吧

%%% 介绍RL
% 根据状态做出动作的强化学习正适合用来解决探索和利用困境
Reinforcement Learning (RL) that selects actions according to the state is good at solving Exploration vs Exploitation dilemma.
% 介绍强化学习
An RL agent interacts with its environment in discrete time steps. At each time $t$, the agent receives the current state $s_t$ and reward $r_t$. It then chooses an action $a_t$ from the set of available actions, which is subsequently sent to the environment. The environment moves to a new state $s_{t+1}$ and the reward $r_{t+1}$ associated with the transition $(s_t, a_t, s_{t+1})$ is determined. The goal of a reinforcement learning agent is to learn a policy $\pi(a, s)=\operatorname{Pr}\left(a_{t}=a \mid s_{t}=s\right)$ which maximizes the expected cumulative reward.

%%% 将RL应用到AOS
% 在自适应算子选择问题中，我们可以将多目标优化问题和当前的种群作为环境，将算子作为动作，将应用算子后种群发生的变化作为奖励，
In the AOS problems, we take MOPs and the current population as environment, take the population characteristics as states, take the operators as actions and the improvement of population fitness after the application of the operator is regarded as the reward.
% 我们会发现遗传算法和强化学习在概念和结构上有相似性
Viewed in this light, RL can be regarded as similar to GA in its evolution/learning structure, which causes high conceptual and structural compatibility between RL and GA.

%%% 介绍本文
% 我们的算法可以根据不同的个体推荐不同的算子，做到更细粒度的控制，而且分别推荐交叉和变异实现更彻底的控制。
In this paper, we proposed an AOS method based on RL, which can recommend different operators according to the individual to achieve more fine-grained control and recommend crossover operators and mutation operators respectively to achieve better results.

\TODO{The remainder of this paper is organized as follows.}

\IEEEpubidadjcol

\section{Background and Related Work}
%%% 进化算法的效果和产生子代的策略有关，为了充分利用不同算子，最近几年提出了很多自适应算子选择的策略。
The performance of EAs mainly depends on its breeding Offspring strategy, to take full advantage of several effective operators, many adaptive operator selection strategies have been proposed in recent years.
% 这一章节介绍AOS用到的概念和背景知识，和一些相关的工作。
This section provides some basic concepts, background knowledge and some related work about AOS.

\subsection{Multi-objective Optimization and MOEA/D}
%%% 介绍多目标优化和MOEAD
Multi-objective optimization problems are usually formulated as follows:

$$
  \begin{array}{ll}
    \text { Minimize: }   & \mathrm{f}=\left\{f_{1}(\mathrm{x}), \ldots, f_{M}(\mathrm{x})\right\} \\
    \text { Subject to: } & l_{d} \leq x_{d} \leq u_{d} \qquad d=1, \ldots, D
  \end{array}
$$

%%% x是决策变量，f是一组互相冲突的目标函数，由于目标是冲突的，所以不存在一个解可以在所有目标上达到最优，
Where $x = (x_1, \cdots , x_D)$ is a decision variable vector of length $D$, $l$ and $u$ are the lower and upper bounds of the decision variables. $f$ consists of $M$ conflicting objective functions. Since the objectives are conflicting, there is no one solution that is optimal for all objectives.
% 所有可能的解构成决策空间，决策空间可以由目标函数对应到目标空间，在目标空间中，如果一个解在所有目标上都不比另一个差，且至少在一个目标上比另一个好，我们就称为一个解支配另一个解。
All possible decision variables constitute the decision space and objective functions can be mapped from the objective space to the decision space\cite{gonccalves2017adaptive}. In the objective space, one solution is said to dominate the other solution if it is no worse than the other solution on all objectives and is better than the other solution on at least one objective.
% 当一个解不被支配时就是Pareto最优解
A solution $x^*$ is said to be Pareto-optimal when no other solution can dominate $x^*$. All the Pareto-optimal solutions constitute the Pareto set, and the image of the Pareto set in objective space is called the Pareto front \cite{deb2001multi}.

%%% 主要框架是MOEA/D，所以肯定要大略介绍下
% 将AOS应用到多目标上的工作很少，因为多目标很难定量的描述子代相对于父代适应度的提高，
Very little work has been done to apply AOS to multi-objective evolutionary algorithms since it is difficult to measure the increase in fitness of the offspring relative to the parent.
% 在论文中，我们使用MOEA/D作为主要的多目标优化框架，MOEAD的基本思想是将多目标问题分解成多个单目标的子问题,因此将AOS与MOEA/D结合是适合的。
In this paper, we use MOEA/D \cite{moead} as the evolutionary multi-objective optimization framework. The basic idea of MOEA/D is to decompose a multi-objective optimization problem into several scalar optimization subproblems and optimizes them together. Thus, it is appropriate to combine AOS with MOEA/D.

\subsection{Adaptive Operator Selection}
%%% 介绍自适应算子选择
% 自适应算子选择是通过在线控制的方式，在搜索过程中选择合适的算子,选择的依据是算子在之前的表现.
AOS selects the appropriate operator from a candidate set of recombination and mutation operators pool for the individual in the search process \cite{hitomi2016classification}. The selection is based on the previous performance of the operator.
% --- TODO: 介绍MOEAD与AOS结合后的处理流程
% AOS主要有两个任务，适应度赋值(评价算子)和算子选择(依据评价做出选择)。
%%%%%%%%%%%%%%%%%% 补充图片，介绍适应度赋值和算子选择的联系
AOS requires two tasks to be solved, the \textit{Credit Assignment} strategy that defines how to evaluate an operator based on its performance in the search process and the \textit{Operator Selection} strategy that determines the next operator according to the quality of the operator.

\subsubsection{Credit Assignment}
% ---- 需要添加related work！！！！！！！！！！
%%% 信誉分配
% 最常用的信誉分配方法是根据子代相对于父代的适应度提高来确定
The most common method of credit assignment rewards an operator for its performance to improve the fitness of an offspring over its parent, in \cite{lin2016adaptive} the fitness improvement rates (FIR) is obtained with the used operator, as defined in equation (\ref{eq: fir1}).

\begin{equation}
  \text{FIR}_{i}=\frac{pf_{i}-cf_{i}}{pf_{i}} \label{eq: fir1}
\end{equation}

where $pf_i$ and $cf_i$ are the fitness values corresponding to parent and offspring respectively.
% 由于有写目标函数值的量级不同，如果我们知道真实最优值，一般用父代与真实最优值的差做分母。
In order to track the dynamics of search process, ExAOS \cite{fialho2008extreme} use a sliding window to save recent fitness improvement rates produced by the operators,
% 有观点认为稀少但巨大的改进比频繁但微小的改进更有用
some people believe the operators that make rare but large improvements are more important than operators that make frequent and small improvements \cite{fialho2009dynamic},
% 滑动窗口里存提高率，reward是滑动窗口里operator对应的最大值，
the operator reward is set to the maximal fitness improvement in the time windows, credit assignment strategy rewards the operators that generate a high quality of offspring.
% 除了上面的奖励方法，还有文章根据分布性奖励算子
In addition there are some strategies reward operators that increase the fitness of the solution while maintaining the diversity of the solution set \cite{auer2002finite}.

% 大多数的AOS研究都是关于单目标的，因为多目标很难做信誉赋值，而且也不容易比较解
Most of the studies on credit assignment in AOS are for single-objective problems, such as \cite{fialho2009dynamic,fialho2009dynamic,cowling2000hyperheuristic}, and there has been little work done on AOS in multi-objective evolutionary computation because it is very difficult to define the credit assignment and compare one solution to another.

% 现有的多目标信誉赋值在比较个体和赋值方式上有所不同
The existing multi-objective credit assignment is different in which individuals should be compared and assigning strategies, most of the AOS for MOPs is based on different evolution.
% 有些工作是基于支配关系的，
Some previous works reward operators based on the dominant relationships,
% MOSaDE依据帕累托支配关系和拥挤距离来奖励算子。
such as MOSaDE \cite{huang2007multi} rewards an operator if it is better than incumbent solution in terms of Pareto-dominance and crowding distance.
In MCHH \cite{mcclymont2011markov}, the reward of operator is determined by the offspring it produces that dominate many solutions from the previous generation. Adap-MODE \cite{li2011multi} rewards an operator according to the fitness improvement of a solution, where fitness is the weighted sum of Pareto metric and density metric.
% 有一些基于支配关系奖励算子的工作。比如在AMALGAM中，
% 最近的一些工作大多是基于分解的多目标算法,比如FRRMAB，根据基于分解的适应值函数评价算子对于目标函数值的提高能力
Most of the recent work is based on decomposition of multi-objective algorithm, such as MOEA/D-FRRMAB \cite{frrmab}, which reward operator according to the improvement of decomposition-based fitness value compared with the neighboring subproblems.
% B-AOS 双标准的辅助的AOS，一个指标是收敛性，另一个指标是分布性。
B-AOS \cite{lin2021decomposition} proposed a bicriteria assisted adaptive operator selection strategy, in which one criterion reflecting the convergence according to the Pareto dominant relation and another criterion reflecting the diversity according to the crowding distance.
% 算法在前期比较注重分布性，在后期更加注重收敛性。
The algorithm keeps the distribution of the population in the early stage, and pays more attention to the convergence of the population in the later stage.
% 也有一些基于支配关系的工作，
On the other hand, AMALGAM \cite{vrugt2007improved} rewards the operator according to the number of solutions it contributes to the population.
% 通常来讲，有三种基于支配关系的奖励方式。
Generally speaking, there are three different credit assignments based on dominance, offspring-dominates-parent (ODP), Pareto front (PF), and Pareto rank (PR) \cite{hitomi2015effect}.

\subsubsection{Operator Selection}
% 和信誉赋值相比，
% Compared with credit assignment, the operator selection 
% 信誉赋值之后就可以根据算子的信誉为个体推荐合适的算子，
After credit assignment, we can recommend appropriate operators for individuals according to the credit of operators, but we can't choose the operator with the highest credit directly, because it will make us lose diversity. On the one hand, we need to give more opportunities to operators with better track records, on the other hand, we need to explore poor operators, since an operator might perform significantly differently in the future search stages, we called this difficult as exploration versus exploitation (EvE) dilemma.

% 一个常用的方法是概率匹配算法
A common method is probability matching (PM) algorithm \cite{thierens2007adaptive}, each operator $o_i$ in the operator pool $O$ has a corresponding probability value $p_{i,t}$ as generation $t$, $p$ is determined by the quality $q_{i,t}$, $q$ is updated according to the reward $r_{i,t}$ received during the credit assignment phase, and the formula is as follows:
$$
  q_{i,t+1} = q_{i,t} + \alpha(r_{i,t} - q_{i,t})
$$
% 这是利用时序差分法，根据算法的reward估计算子的质量。
with the learning rate $\alpha \in (0,1]$, using the Temporal-Different method to estimate the quality of operator.
% 然而，如果有算子在当前的概率=0，那么他在之后就永远不会被选中，quality也不会被更新
Once a quality $a_{i,t}$ becomes equal to 0, the operator will never be selected in the future and its quality can no longer be updated.
% 在非平稳的环境中，该算子可能在之后会有更好的表现，所以
In a non-stationary environment, the operator might perform better in a future stage of the search process.
% 为了避免有的算子概率太小以至于长时间不被选择，PM设定一个最小选择概率称为 Pmin,
So in order to avoid the operator probability is too small to be selected, PM designs a minimum probability, $p_{min}: 1 > p_{min} > 0$, for exploring currently underperforming operator. Since the total probability is 1, then the maximum probability of an operator being selected is $p_{max} = 1-(k-1) P_{min}$ where $k$ is the number of operators, thus we can get the update rule of $p_{i,t}$:
$$
  p_{i,t+1} = p_{min} + (1-k \cdot p_{min}) \frac{q_{i,t+1}}{\sum_{i=1}^k q_{i,t+1}}
$$



\subsection{Reinforcement Learning}
% EvE困境经常被建模为MAB问题，有很多的算法可以解决MAB问题，比如UCB，但AOS并不是vanilla的MAB，由于每个臂的收益分布会随着时间变化，所以它是一个动态MAB问题
% li ke使用滑动窗口解决D-MAB问题，

\section{The Proposed Algorithm}
test
\subsection{Frassmework of MOEA/PSL}
test
\subsection{Pareto Optimal Subspace Learning and Offspring Generation in MOEA/PSL}

\subsection{Parameter Adaptation Strategy in MOEA/PSL}

\subsection{Computational Complexity of MOEA/PSL}


\section{Empirical Studies}


\subsection{Comparative Algorithms}

\subsection{Test Problems}

\subsection{Results on Benchmark Problems}

\subsection{Results on Real-World Problems}

\subsection{Effectiveness of the Two Neural Networks in MOEA/PSL}\label{sec:twoNNs}

\subsection{Effectiveness of the Parameter Adaptation Strategy in MOEA/PSL}

\section{Conclusions}


 (e.g., deep neural network training)

\ifCLASSOPTIONcaptionsoff
  \newpage
\fi

\bibliographystyle{IEEEtran}
\bibliography{references}

% \iffalse
%   \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{XingyiZhang.eps}}]{Xingyi Zhang}
%     received the B.Sc. from Fuyang Normal College in 2003, and the M.Sc. in 2006 and Ph.D. in 2009
%     both from Huazhong University of Science and Technology. Currently, he is
%     an associate professor in the School of Computer Science and Technology, Anhui University. His main research interests
%     include unconventional models and algorithms of computation, multi-objective optimization and membrane computing.
%   \end{IEEEbiography}
% \fi

\end{document}
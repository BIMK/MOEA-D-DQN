\documentclass[journal]{IEEEtran}

\usepackage{palatino,epsfig,latexsym,cite,graphicx,amsmath,amssymb,amsfonts,multirow,booktabs,color,soul}
\usepackage{algorithmic,url}
\usepackage{float}
\usepackage{threeparttable}
\usepackage{amsmath}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\def\MR2{\multirow{2}[2]{*}}
\def\Hei{\hl}
\definecolor{hl}{rgb}{0.75,0.75,0.75}
\newcommand{\XG}[1]{\textcolor[rgb]{0.00,0.00,1.00}{#1}}
\newcommand{\TODO}[1]{\textcolor[rgb]{1.00,0.40,0.22}{#1}}
\sethlcolor{hl}
\usepackage{makecell}
\usepackage[caption=false,font=footnotesize]{subfig}
\renewcommand{\algorithmicrequire}{ \textbf{Input:}}      %Use Input in the format of Algorithm
\renewcommand{\algorithmicensure}{ \textbf{Output:}}     %Use Output in the format of Algorithm
\DeclareMathOperator*{\argmin}{argmin}

%\setlength{\abovecaptionskip}{-5pt}
\setlength{\floatsep}{12pt  minus 4pt}
\setlength{\textfloatsep}{13pt minus 4pt}
\renewcommand{\textfraction}{0.1}
\renewcommand{\topfraction}{0.9}
\setlength\arraycolsep{2pt}

\begin{document}

\title{{Adaptation Operator Selection with Reinforcement Learning for Multi-objective Evolutionary}
  \thanks{Thanks}}
\author{
  authors,
}% <-this % stops a space

% \markboth{IEEE Transactions on Cybernetics,~Vol.~, No.~, month~year}
% {Tian \MakeLowercase{\textit{et al.}}: SOLVING LMOPS WITH SPARSE OPTIMAL SOLUTIONS VIA UNSUPERVISED NNS}

\IEEEpubid{0000--0000/00\$00.00~\copyright~0000 IEEE}

\maketitle

\begin{abstract}
  Evolutionary Algorithms (EAs) are stochastic optimization algorithms which are greatly influenced by operators, different reproduction operators have various search range.
  Adaptive operator selection (AOS) provides the on-line control of the evolutionary algorithms that which operator should be chosen for the current individual.
  This paper proposes an AOS strategy based on Double Q-learning (DDQN), a deep Reinforcement Learning (RL) algorithm in discrete action spaces, to detect the operator should be applied.
  The two main components of AOS, Credit Assignment and Operator Selection, correspond to Reward and Action of Markov Decision Process (MDP) and the State of MDP can be represented by the features of the current individual. According to the above rules, the operator selection problem is modeled as MDP and can be solved by reinforcement learning. In the evolutionary process, RL will select an operator for each individual and update the behavioral strategy according to the effect of this selection. Experimental results on \TODO{supplement experimental instructions}.
\end{abstract}

\begin{IEEEkeywords}
  Multi-objective optimization, Reinforcement Learning, Credit assignment
\end{IEEEkeywords}

\section{Introduction}
%%% 介绍下进化多目标优化
% 由于多个目标间通常互相冲突，没有一个全局最优解，一般会有一个PF
\IEEEPARstart{M}{ultl-object} optimization problems (MOPs) are characterized by multiple objectives that often conflict with each other\cite{zhang2014efficient}. In general, MOPs have no single optimal solution on all objectives; instead, a number of solutions are optimal for different objectives, known as Pareto-optimal solutions.
% All the pareto optimal solutions constitute the Pareto set and the set of all pareto optimal objective vectors is the Pareto front \cite{deb2001multi}.

%%% 介绍进化算法
% 进化算法可以解决很多多目标优化问题，最近几十年有了很大的发展，
Multi-object evolutionary algorithms have attracted great attention over the past few decades and achieved outstanding performance in solving various kinds of MOPs \cite{fialho2010adaptive},
% 人们提出了很多新的算法，主要有3类，基于支配关系的，基于分解的和基于指标的算法
a number of algorithms have been proposed, e.g., indicator-based evolutionary algorithm (IBEA) \cite{IBEA}, multi-objective evolutionary algorithm based on decomposition (MOEA/D) \cite{moead} and fast non-domainated sorting genetic algorithm (NSGA-II) \cite{nsga2}.
% 生成算子对进化算法的性能有很大的影响，有的算子注重探索能力，我们称为全局算子，有的注重局部的收敛，我们称为局部算子,那么如何平衡算法的探索和利用就成了人们研究的对象
In these algorithms, the reproduction operators have great influence on the performance.
Generally speaking, different operators have different applications, some operators tend to explore unknown regions to find global optimal solutions, called global operators; and the others tend to converge quickly with population's information, called local operators.
% 所以我们要平衡探索和利用算子
In order to accelerate the process of evolution, balanced exploration and exploitation became an important topic.

%%% 介绍AOS
% AOS就是为个体推荐合适的算子，在线控制
Adaptive operator selection (AOS) applies appropriate operators for individuals by online control, while solving the problem, based on recent performance of operators \cite{li2011multi}.
% 包含credit assignment和operator selection 两部分，
An AOS algorithm usually consists of \textit{credit assignment} and \textit{operator selection}, credit assignment is to decide how much credit should be assigned to an operator based on its recent performance in the search process, operator selection is to select an operator to use at the next time point based on these current reward values \cite{frrmab}.
% 和credit assignment 尽量准确评价算子近期的效果不同，operator selection 需要平衡探索和利用，即既需要选择尽量表现好的算子来提高算法效果，又需要给表现不好的算子一些机会，因为它们在之后有可能变好。
Different from credit assignment, which only needs to evaluate operator performance as accurately as possible, operator selection should balance exploration and exploitation: one hopes to give more chances to the best operators found so far in order to maximize the cumulative improvement (exploitation), but also needs to explore poor operators in the future search (exploration).
% 通常有MAB，UCB，可以和MOEAD结合应用到多目标问题上。
% 但是由于环境是不稳定的，一开始好的算子不一定在最后好，所以就有动态MAB或者动态汤普森采样等。
% 写到related work里吧

%%% 介绍RL
% RL正适合用来解决环境的动态变化和面临的探索利用困境问题，
Reinforcement Learning (RL) that selects actions according to the state is good at solving Exploration vs Exploitation dilemma.
% RL。。。。




\begin{equation}
  \begin{array}{ll}
    \text { Minimize: }   & F=\left\{f_{1}(x), \ldots, f_{M}(x)\right\} \\
    \text { Subject to: } & x \in \Omega
  \end{array}
\end{equation}






\IEEEpubidadjcol

The second paragraph...
\section{Related Work}

\subsection{Subsection Name ... }
yihang

ling hang

\subsection{the second subsection name...}
test
\section{The Proposed Algorithm}
test
\subsection{Framework of MOEA/PSL}
test
\subsection{Pareto Optimal Subspace Learning and Offspring Generation in MOEA/PSL}

\subsection{Parameter Adaptation Strategy in MOEA/PSL}

\subsection{Computational Complexity of MOEA/PSL}


\section{Empirical Studies}


\subsection{Comparative Algorithms}

\subsection{Test Problems}

\subsection{Results on Benchmark Problems}

\subsection{Results on Real-World Problems}

\subsection{Effectiveness of the Two Neural Networks in MOEA/PSL}\label{sec:twoNNs}

\subsection{Effectiveness of the Parameter Adaptation Strategy in MOEA/PSL}

\section{Conclusions}


 (e.g., deep neural network training)

\ifCLASSOPTIONcaptionsoff
  \newpage
\fi

\bibliographystyle{IEEEtran}
\bibliography{references}

% \iffalse
%   \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{XingyiZhang.eps}}]{Xingyi Zhang}
%     received the B.Sc. from Fuyang Normal College in 2003, and the M.Sc. in 2006 and Ph.D. in 2009
%     both from Huazhong University of Science and Technology. Currently, he is
%     an associate professor in the School of Computer Science and Technology, Anhui University. His main research interests
%     include unconventional models and algorithms of computation, multi-objective optimization and membrane computing.
%   \end{IEEEbiography}
% \fi

\end{document}
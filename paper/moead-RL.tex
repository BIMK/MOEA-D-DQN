\documentclass[journal]{IEEEtran}

\usepackage{palatino,epsfig,latexsym,cite,graphicx,amsmath,amssymb,amsfonts,multirow,booktabs,color,soul}
\usepackage{algorithmic,url}
\usepackage{float}
\usepackage{threeparttable}
\usepackage{amsmath}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\def\MR2{\multirow{2}[2]{*}}
\def\Hei{\hl}
\definecolor{hl}{rgb}{0.75,0.75,0.75}
\newcommand{\XG}[1]{\textcolor[rgb]{0.00,0.00,1.00}{#1}}
\newcommand{\TODO}[1]{\textcolor[rgb]{1.00,0.40,0.22}{#1}}
\sethlcolor{hl}
\usepackage{makecell}
\usepackage[caption=false,font=footnotesize]{subfig}
\renewcommand{\algorithmicrequire}{ \textbf{Input:}}      %Use Input in the format of Algorithm
\renewcommand{\algorithmicensure}{ \textbf{Output:}}     %Use Output in the format of Algorithm
\DeclareMathOperator*{\argmin}{argmin}

%\setlength{\abovecaptionskip}{-5pt}
\setlength{\floatsep}{12pt  minus 4pt}
\setlength{\textfloatsep}{13pt minus 4pt}
\renewcommand{\textfraction}{0.1}
\renewcommand{\topfraction}{0.9}
\setlength\arraycolsep{2pt}

\begin{document}

\title{{Adaptation Operator Selection with Reinforcement Learning for Multi-objective Evolutionary}
  \thanks{Thanks}}
\author{
  authors,
}% <-this % stops a space

% \markboth{IEEE Transactions on Cybernetics,~Vol.~, No.~, month~year}
% {Tian \MakeLowercase{\textit{et al.}}: SOLVING LMOPS WITH SPARSE OPTIMAL SOLUTIONS VIA UNSUPERVISED NNS}

\IEEEpubid{0000--0000/00\$00.00~\copyright~0000 IEEE}

\maketitle

\begin{abstract}
  Evolutionary Algorithms (EAs) are stochastic optimization algorithms which are greatly influenced by operators, different reproduction operators have various search range.
  Adaptive operator selection (AOS) provides the on-line control of the evolutionary algorithms that which operator should be chosen for the current individual.
  This paper proposes an AOS strategy based on Double Q-learning (DDQN), a deep Reinforcement Learning (RL) algorithm in discrete action spaces, to detect the operator should be applied.
  The two main components of AOS, Credit Assignment and Operator Selection, correspond to Reward and Action of Markov Decision Process (MDP) and the State of MDP can be represented by the features of the current individual. According to the above rules, the operator selection problem is modeled as MDP and can be solved by reinforcement learning. In the evolutionary process, RL will select an operator for each individual and update the behavioral strategy according to the effect of this selection. Experimental results on \TODO{supplement experimental instructions}.
\end{abstract}

\begin{IEEEkeywords}
  Multi-objective optimization, Reinforcement Learning, Credit assignment
\end{IEEEkeywords}

\section{Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%      需要在introduction里说明我的算法，包括主要的过程，实验部分，和其它RL的相关知识
\TODO{ need to supplement the flow of algorithm}

%%% 介绍下进化多目标优化
% 由于多个目标间通常互相冲突，没有一个全局最优解，一般会有一个PF
\IEEEPARstart{M}{ultl-object} optimization problems (MOPs) are characterized by multiple objectives that often conflict with each other\cite{zhang2014efficient}. In general, MOPs have no single optimal solution on all objectives; instead, a number of solutions are optimal for different objectives, known as Pareto-optimal solutions.
% All the Pareto optimal solutions constitute the Pareto set and the set of all Pareto optimal objective vectors is the Pareto front \cite{deb2001multi}.

%%% 介绍进化算法
% 进化算法可以解决很多多目标优化问题，最近几十年有了很大的发展，
Multi-object evolutionary algorithms have attracted great attention over the past few decades and achieved outstanding performance in solving various kinds of MOPs \cite{fialho2010adaptive},
% 人们提出了很多新的算法，主要有3类，基于支配关系的，基于分解的和基于指标的算法
a number of algorithms have been proposed, e.g., indicator-based evolutionary algorithm (IBEA) \cite{IBEA}, the multi-objective evolutionary algorithm based on decomposition (MOEA/D) \cite{moead} and fast non-dominated sorting genetic algorithm (NSGA-II) \cite{nsga2}.
% 生成算子对进化算法的性能有很大的影响，有的算子注重探索能力，我们称为全局算子，有的注重局部的收敛，我们称为局部算子,那么如何平衡算法的探索和利用就成了人们研究的对象
In these algorithms, the reproduction operators have a great influence on the performance.
Generally speaking, different operators have different applications, some operators tend to explore unknown regions to find global optimal solutions, called global operators; and the others tend to converge quickly with population information, called local operators.
% 所以我们要平衡探索和利用算子
In order to accelerate the process of evolution, balanced exploration and exploitation became an important topic.

%%% 介绍AOS
% AOS就是为个体推荐合适的算子，在线控制
Adaptive operator selection (AOS) applies appropriate operators for individuals by online control while solving the problem, based on the recent performance of operators \cite{li2011multi}.
% 包含credit assignment和operator selection 两部分，
An AOS algorithm usually consists of \textit{credit assignment} and \textit{operator selection} \cite{li2013evolving}, credit assignment is to decide how much credit should be assigned to an operator based on its recent performance in the search process, operator selection is to select an operator to use at the next time point based on these current reward values \cite{frrmab}.
% 和credit assignment 尽量准确评价算子近期的效果不同，operator selection 需要平衡探索和利用，即既需要选择尽量表现好的算子来提高算法效果，又需要给表现不好的算子一些机会，因为它们在之后有可能变好。
Different from credit assignment, which only needs to evaluate operator performance as accurately as possible, operator selection should balance exploration and exploitation: one hopes to give more chances to the best operators found so far in order to maximize the cumulative improvement (exploitation), but also needs to explore poor operators in the future search (exploration).
% 通常有MAB，UCB，可以和MOEAD结合应用到多目标问题上。
% 但是由于环境是不稳定的，一开始好的算子不一定在最后好，所以就有动态MAB或者动态汤普森采样等。
% 写到related work里吧

%%% 介绍RL
% 根据状态做出动作的强化学习正适合用来解决探索和利用困境
Reinforcement Learning (RL) that selects actions according to the state is good at solving Exploration vs Exploitation dilemma.
% 介绍强化学习
An RL agent interacts with its environment in discrete time steps. At each time $t$, the agent receives the current state $s_t$ and reward $r_t$. It then chooses an action $a_t$ from the set of available actions, which is subsequently sent to the environment. The environment moves to a new state $s_{t+1}$ and the reward $r_{t+1}$ associated with the transition $(s_t, a_t, s_{t+1})$ is determined. The goal of a reinforcement learning agent is to learn a policy $\pi(a, s)=\operatorname{Pr}\left(a_{t}=a \mid s_{t}=s\right)$ which maximizes the expected cumulative reward.

%%% 将RL应用到AOS
% 在自适应算子选择问题中，我们可以将多目标优化问题和当前的种群作为环境，将算子作为动作，将应用算子后种群发生的变化作为奖励，
In the AOS problems, we take MOPs and the current population as environment, take the population characteristics as states, take the operators as actions and the improvement of population fitness after the application of the operator is regarded as the reward.
% 我们会发现遗传算法和强化学习在概念和结构上有相似性
Viewed in this light, RL can be regarded as similar to GA in its evolution/learning structure, which causes high conceptual and structural compatibility between RL and GA.

%%% 介绍本文
% 我们的算法可以根据不同的个体推荐不同的算子，做到更细粒度的控制，而且分别推荐交叉和变异实现更彻底的控制。
In this paper, we proposed an AOS method based on RL, which can recommend different operators according to the individual to achieve more fine-grained control and recommend crossover operators and mutation operators respectively to achieve better results.

\TODO{The remainder of this paper is organized as follows.}

\IEEEpubidadjcol

\section{Background and Related Work}
%%% 进化算法的效果和产生子代的策略有关，为了充分利用不同算子，最近几年提出了很多自适应算子选择的策略。
The performance of EAs mainly depends on its breeding Offspring strategy, to take full advantage of several effective operators, many adaptive operator selection strategies have been proposed in recent years.
% 这一章节介绍AOS用到的概念和背景知识，和一些相关的工作。
This section provides some basic concepts, background knowledge and some related work about AOS.

\subsection{Multi-objective Optimization and MOEA/D}
%%% 介绍多目标优化和MOEAD
Multi-objective optimization problems are usually formulated as follows:

\begin{equation}
  \begin{array}{ll}
    \text { Minimize: }   & \mathrm{f}=\left\{f_{1}(\mathrm{x}), \ldots, f_{M}(\mathrm{x})\right\} \\
    \text { Subject to: } & l_{d} \leq x_{d} \leq u_{d} \quad d=1, \ldots, D
  \end{array}
  \label{eq: moea}
\end{equation}

%%% x是决策变量，f是一组互相冲突的目标函数，由于目标是冲突的，所以不存在一个解可以在所有目标上达到最优，
Where $x = (x_1, \cdots , x_D)$ is a decision variable vector of length $D$, $l$ and $u$ are the lower and upper bounds of the decision variables. $f$ consists of $M$ conflicting objective functions. Since the objectives are conflicting, there is no one solution that is optimal for all objectives.
% 所有可能的解构成决策空间，决策空间可以由目标函数对应到目标空间，在目标空间中，如果一个解在所有目标上都不比另一个差，且至少在一个目标上比另一个好，我们就称为一个解支配另一个解。
All possible decision variables constitute the decision space and objective functions can be mapped from the objective space to the decision space\cite{gonccalves2017adaptive}. In the objective space, one solution is said to dominate the other solution if it is no worse than the other solution on all objectives and is better than the other solution on at least one objective.
% 当一个解不被支配时就是Pareto最优解
A solution $x^*$ is said to be Pareto-optimal when no other solution can dominate $x^*$. All the Pareto-optimal solutions constitute the Pareto set, and the image of the Pareto set in objective space is called the Pareto front \cite{deb2001multi}.

%%% 主要框架是MOEA/D，所以肯定要大略介绍下
% 将AOS应用到多目标上的工作很少，因为多目标很难定量的描述子代相对于父代适应度的提高，
Very little work has been done to apply AOS to multi-objective evolutionary algorithms since it is difficult to measure the increase in fitness of the offspring relative to the parent.
% 在论文中，我们使用MOEA/D作为主要的多目标优化框架，MOEAD的基本思想是将多目标问题分解成多个单目标的子问题,因此将AOS与MOEA/D结合是适合的。
In this paper, we use MOEA/D \cite{moead} as the evolutionary multi-objective optimization framework. The basic idea of MOEA/D is to decompose a multi-objective optimization problem into several scalar optimization subproblems and optimizes them together. Thus, it is appropriate to combine AOS with MOEA/D.
%%%%%%%%%%% moead-de
\TODO{MOEAD-DE}

\subsection{Adaptive Operator Selection}
%%% 介绍自适应算子选择
% 自适应算子选择是通过在线控制的方式，在搜索过程中选择合适的算子,选择的依据是算子在之前的表现.
AOS selects the appropriate operator from a candidate set of recombination and mutation operators pool for the individual in the search process \cite{hitomi2016classification}. The selection is based on the previous performance of the operator.
% --- TODO: 介绍MOEAD与AOS结合后的处理流程
% AOS主要有两个任务，适应度赋值(评价算子)和算子选择(依据评价做出选择)。
%%%%%%%%%%%%%%%%%% 补充图片，介绍适应度赋值和算子选择的联系
AOS requires two tasks to be solved, the \textit{Credit Assignment} strategy that defines how to evaluate an operator based on its performance in the search process and the \textit{Operator Selection} strategy that determines the next operator according to the quality of the operator.

\subsubsection{Credit Assignment}
% ---- 需要添加related work！！！！！！！！！！
%%% 信誉分配
% 最常用的信誉分配方法是根据子代相对于父代的适应度提高来确定
The most common method of credit assignment rewards an operator for its performance to improve the fitness of an offspring over its parent, in \cite{lin2016adaptive} the fitness improvement rates (FIR) is obtained with the used operator, as defined in equation (\ref{eq: fir1}).

\begin{equation}
  \text{FIR}_{i}=\frac{pf_{i}-cf_{i}}{pf_{i}} \label{eq: fir1}
\end{equation}

where $pf_i$ and $cf_i$ are the fitness values corresponding to parent and offspring respectively.
% 由于有写目标函数值的量级不同，如果我们知道真实最优值，一般用父代与真实最优值的差做分母。
In order to track the dynamics of search process, ExAOS \cite{fialho2008extreme} use a sliding window to save recent fitness improvement rates produced by the operators,
% 有观点认为稀少但巨大的改进比频繁但微小的改进更有用
some people believe the operators that make rare but large improvements are more important than operators that make frequent and small improvements \cite{fialho2009dynamic},
% 滑动窗口里存提高率，reward是滑动窗口里operator对应的最大值，
the operator reward is set to the maximal fitness improvement in the time windows, credit assignment strategy rewards the operators that generate a high quality of offspring.
% 除了上面的奖励方法，还有文章根据分布性奖励算子
In addition there are some strategies reward operators that increase the fitness of the solution while maintaining the diversity of the solution set \cite{auer2002finite}.

% 大多数的AOS研究都是关于单目标的，因为多目标很难做信誉赋值，而且也不容易比较解
Most of the studies on credit assignment in AOS are for single-objective problems, such as \cite{fialho2009dynamic,fialho2009dynamic,cowling2000hyperheuristic}, and there has been little work done on AOS in multi-objective evolutionary computation because it is very difficult to define the credit assignment and compare one solution to another.

% 现有的多目标信誉赋值在比较个体和赋值方式上有所不同
The existing multi-objective credit assignment is different in which individuals should be compared and assigning strategies, most of the AOS for MOPs is based on different evolution.
% 有些工作是基于支配关系的，
Some previous works reward operators based on the dominant relationships,
% MOSaDE依据帕累托支配关系和拥挤距离来奖励算子。
such as MOSaDE \cite{huang2007multi} rewards an operator if it is better than incumbent solution in terms of Pareto-dominance and crowding distance.
In MCHH \cite{mcclymont2011markov}, the reward of operator is determined by the offspring it produces that dominate many solutions from the previous generation. Adap-MODE \cite{li2011multi} rewards an operator according to the fitness improvement of a solution, where fitness is the weighted sum of Pareto metric and density metric.
% 有一些基于支配关系奖励算子的工作。比如在AMALGAM中，
% 最近的一些工作大多是基于分解的多目标算法,比如FRRMAB，根据基于分解的适应值函数评价算子对于目标函数值的提高能力
Most of the recent work is based on decomposition of multi-objective algorithm, such as MOEA/D-FRRMAB \cite{frrmab}, which reward operator according to the improvement of decomposition-based fitness value compared with the neighboring subproblems.
% B-AOS 双标准的辅助的AOS，一个指标是收敛性，另一个指标是分布性。
B-AOS \cite{lin2021decomposition} proposed a bicriteria assisted adaptive operator selection strategy, in which one criterion reflecting the convergence according to the Pareto dominant relation and another criterion reflecting the diversity according to the crowding distance.
% 算法在前期比较注重分布性，在后期更加注重收敛性。
The algorithm keeps the distribution of the population in the early stage, and pays more attention to the convergence of the population in the later stage.
% 也有一些基于支配关系的工作，
On the other hand, AMALGAM \cite{vrugt2007improved} rewards the operator according to the number of solutions it contributes to the population.
% 通常来讲，有三种基于支配关系的奖励方式。
Generally speaking, there are three different credit assignments based on dominance, offspring-dominates-parent (ODP), Pareto front (PF), and Pareto rank (PR) \cite{hitomi2015effect}.

\subsubsection{Operator Selection}
% 和信誉赋值相比，
% Compared with credit assignment, the operator selection 
% 信誉赋值之后就可以根据算子的信誉为个体推荐合适的算子，
After credit assignment, we can recommend appropriate operators for individuals according to the credit of operators, but we can't choose the operator with the highest credit directly, because it will make us lose diversity. On the one hand, we need to give more opportunities to operators with better track records, on the other hand, we need to explore poor operators, since an operator might perform significantly differently in the future search stages, we called this difficult as exploration versus exploitation (EvE) dilemma.

% 一个常用的方法是概率匹配算法
A common method is probability matching (PM) algorithm \cite{thierens2007adaptive}, each operator $o_i$ in the operator pool $O$ has a corresponding probability value $p_{i,t}$ as generation $t$, $p$ is determined by the quality $q_{i,t}$, $q$ is updated according to the reward $r_{i,t}$ received during the credit assignment phase, and the formula is as follows:
\begin{equation}
  q_{i,t+1} = q_{i,t} + \alpha(r_{i,t} - q_{i,t})
  \label{eq: pm_q}
\end{equation}
% 这是利用时序差分法，根据算法的reward估计算子的质量。
with the learning rate $\alpha \in (0,1]$, using the Temporal-Different method to estimate the quality of operator.
% 然而，如果有算子在当前的概率=0，那么他在之后就永远不会被选中，quality也不会被更新
Once a quality $a_{i,t}$ becomes equal to 0, the operator will never be selected in the future and its quality can no longer be updated.
% 在非平稳的环境中，该算子可能在之后会有更好的表现，所以
In a non-stationary environment, the operator might perform better in a future stage of the search process.
% 为了避免有的算子概率太小以至于长时间不被选择，PM设定一个最小选择概率称为 Pmin,
So in order to avoid the operator probability is too small to be selected, PM designs a minimal probability, $p_{min}: 1 > p_{min} > 0$, for exploring currently underperforming operator. Since the total probability is 1, thus we can get the update rule of $p_{i,t}$:
\begin{equation}
  p_{i,t+1} = p_{min} + (1-k \cdot p_{min}) \frac{q_{i,t+1}}{\sum_{i=1}^k q_{i,t+1}}
  \label{eq: pm_p}
\end{equation}
Where $k$ is the number of operators.
% PM在很多差算子的情况下很难获得好的结果, 因为多个差的算子的和也是很大的。
When there are many mediocre operators and only only one efficient operator, it is difficult for PM algorithm to achieve high performance, because the minimal selection probability for each operator is $p_{min}$, the sum of selection probabilities from many mediocre operators has a significant advantage over the high performance operator.
% 引入AP方法，

Another method based on probability is Adaptive pursuit (AP) \cite{thierens2007adaptive}, adopts the winner-take-all strategy to allocate the best operator with higher probability, the pursuit method increases the probability of the best operator $a^*$ being selected and reduces the probabilities of other operators.
% AP的过程介绍
Like PM, AP also designs the minimum probability $p_{min}$, while an operator that works well can obtain the maximum selection probability $p_{max} = 1-(k-1)\times p_{min}$.
AP pursues the operator $o^*$ with the maximal estimated reward as shown in \ref{eq: best_o}, to avoid the disadvantages of PM, AP increases the selection probability of the best operator $p_{o^*,t}$ and decreases the other probabilities $p_{o,t}, \forall o \neq o^*$.
\begin{equation}
  p_{i, t+1}=\left\{\begin{array}{ll}
    p_{i, t}+\beta \cdot\left(p_{\max }-p_{o, t}\right) & \text { if } o=o^{*} \\
    p_{i, t}+\beta \cdot\left(p_{\min }-p_{o, t}\right) & \text { otherwise }
  \end{array}\right.
  \label{eq: AP}
\end{equation}
Where $\beta \in (0,1]$ is the learning rate.
% 搜索过程中，算子的性能可能是动态变化的，因此在算子选择中最困难的就是平衡探索和利用。
% The performance of operators may be dynamic in the search process, so the most difficult problem in operator selection is to balance exploration and exploitation.
% 由于进化过程是动态的，算子的性能在不同阶段可能有很大的差异，AP和PM可能没有办法适应剧烈变化的环境
Due to the dynamic nature of the evolution process, an operator might perform significantly differently in different stages, when credit changes rapidly, PM and AP may not be able to quickly adjust their selection strategies.

% EvE困境经常被建模为MAB问题，有很多的算法可以解决MAB问题，比如UCB，但AOS并不是vanilla的MAB，由于每个臂的收益分布会随着时间变化，所以它是一个动态MAB问题
%%%%%%%%%%%%%%%%%%%  补充MAB介绍
EvE dilemma is often modeled as a Multi-Armed Bandit problem, there are many efficient methods to solve MAB problems,
% UCB 方法
a common method is the upper confidence bound (UCB) \cite{auer2002finite} which guarantees asymptotic optimality with respect to the total cumulative reward. However, UCB solves MAB problem with stationary reward distributions, as previously mentioned, the evolution process is dynamic.
% 基于dynamic MAB的算法
Some effect methods have been proposed to solve the dynamic context based on the original UCB, dynamic multi-armed bandit \cite{dacosta2008adaptive} proceeds by coupling the original MAB technique with a statistical Page-Hinkley test, which enhances the efficient detection of changes in time series.
% sliding mab 使用时间窗
Other operator selectors such as sliding multi-armed bandit (SIMAB) \cite{fialho2010analyzing} using a sliding time window to balance both exploitation and exploration terms.


\subsection{Reinforcement Learning}
% 强化学习介绍
Reinforcement Learning (RL) \cite{sutton2018reinforcement} is an area of machine learning concerned with how intelligent agents ought to take actions in an environment, and the goal of agents is to maximize the notion of cumulative reward \cite{van2012reinforcement}.
% 基于评价性反馈显示了所采取的行动有多好，而不是它是最好的还是最坏的行动。
Feedback based on evaluation shows how good the action is, but not whether it was the best or the worst action passible.
% 强化学习与其他学习类型最重要的区别是，它使用的训练信息是评估所采取的行动，而不是通过给出正确的行动来指导。这就产生了积极探索的需要，明确寻找良好行为的需要。
The most important difference between RL and supervised learning is that it uses training information to evaluate the actions, rather than instructs by giving best actions.
% 因此RL更注重平衡探索和利用
Therefore, RL pays more attention to find a balance between exploration (of uncharted territory) and exploitation (of current knowledge) \cite{kaelbling1996reinforcement}.
% 由于强化学习具有平衡探索和利用的特性，它可以被用来解决D-mab问题
Since RL has the feature of balance EvE, it can be used to solve MAB problem.

% 强化学习的环境通常以马尔科夫决策过程的形式表示
The environment of reinforcement learning is usually expressed in the form of Markov decision process (MDP) \cite{van2012reinforcement}
% MDP是在环境中模拟智能体的随机性策略（policy）与回报的数学模型，
MDP is a mathematical model that simulates the agent's policy and return in the environment.
% 按定义，MDP包含5个模型要素，状态（state）、动作（action）、策略（policy）、奖励（reward）和回报（return
By definition, MDP includes five elements: state, action, policy, reward and return.
% 强化学习把学习看作试探评价过程，Agent选择一个动作用于环境，环境接受该动作后状态发生变化，同时产生一个强化信号(奖或惩)反馈给Agent，Agent根据强化信号和环境当前状态再选择下一个动作，选择的原则是使受到正强化(奖)的概率增大。选择的动作不仅影响立即强化值，而且影响环境下一时刻的状态及最终的强化值。
% 基本的强化学习过程
The typical model of RL can be described as: an agent takes actions according to the state given by environment, the environment accepts the action and returns the reward to the agent, and the goad of RL is to adjust the strategy to maximize the return (the accumulation of rewards over time).
%%%%% 当状态空间很小时，我们可以很容易的画出状态转移过程，此时强化学习的方法类似动态规划，
% 从方法上来讲，强化学习可以分为两类，基于策略的方法和基于值的方法。
RL can be divided into two categories, value-based methods and strategy-based methods.
% 策略是一个条件概率分布，表明在某种状态下执行某个动作的概率。
A policy $\pi$ is a conditional probability distribution $\pi = p(a_t\| s_t; \theta)$, which indicates the probability of taking action $a_t$ in state $s_t$ and $\theta$ is the parameter of policy \cite{sun2021learning}, policy map a state to an action or a distribution over actions, so it can be used in continuous action environment, and the policy optimization is to find an optimal mapping.
% 动作值函数q是表示在状态s下执行动作a可以获得的期望回报，是强化学习的基本概念,Q-learning是经典的基于动作函数优化的方法，一旦我们知道值函数或是状态函数，就可以推导出选择动作的策略。
Action value function $q(s_t,a_t)$ is the expected reward that can be obtained by taking action $a_t$ in state $s_t$, which is the fundamental concept in RL. Q-learning \cite{watkins1992q} is a classical algorithm for learning action value function and once we have optimal action value function, we can derive that the optimal strategy is to choose the action with the largest $q$ value in the current state,
% 只有当动作值函数是有限的时候才能选出合适的动作，因此Q-learning的方法只能用于离散的动作空间
the value-based RL methods can only be used in discrete action space.

% 当状态空间特别巨大时，对值函数的估计是特别困难的，DQN通过深度神经网络稳定训练的动作值函数
When state space is very large, it is difficult to estimate the action function, Deep Q-Network (DQN) \cite{mnih2015human} stabilizes the training of action value function approximation with deep neural networks.
% 但DQN会有过高估计的问题，因此我们采用Double DQN，他通过分离选择和评价
DQN uses the same values to select and to evaluate an action, so if an action value is overestimable, it will be selected and overestimated again. To prevent this, Double Q-learning \cite{hasselt2010double} decouples the evaluation from the selection, futhermore, In Double Deep Q-Network (DDQN) \cite{ddqn}, two neural networks are learned by assigning each experience randomly to update one of the two networks, a target network is designed to select an action and a evaluatino network is designed to generate a Q-value for that action.

% 在这份工作中，我们使用DDQN作为AOS的方法，并为DE算法选择合适的算子。
In this work, we use DDQN as the AOS methos and integrate it into MOEA/D-DE.
% 我们使用在线控制的方法，将强化学习的选择器嵌入到进化算法中，并根据进化的效果给强化学习适当的奖励，根据奖励我们可以更新强化学习的策略，使得在未来获得最高的期望奖励。
We used online control method to embed RL selector into evolutionary algorithm,
according to the effect of evolution, we can give appropriate reward to RL and update the strategy to get the highest expected reward in the future.


% RL的环境通常被描述成马尔科夫决策过程
% The environment is typically stated in the form of a Markov decision process (MDP),

\section{AOS via DDQN}
% 许多论文采用预训练的方式，比如。首先在训练问题上训练多伦，然后在测试问题上使用在线控制的方式，但我们认为不同的问题有不同的最佳算子，而且由于进化路径的不同，在相同的进化阶段也可能有不同的最佳算子，因此AOS难以有迁移性。
% 
% 因为算子的性能在不同的搜索阶段有不同的效果，因此使用离线的方式，先训练后应用的方式是不可靠的。
% 根据问题的特点,不同的算子适合不同的问题，由于问题的特征不能全部包含在RL的状态中，因此在一个问题上表现好的AOS模型不一定在另一个问题上取得好的效果。
According to the characteristics of problems, different operators are suitable for different problems, due to the characteristics can not be included in the RL state, so the AOS model that performs well on one problem may not achieveds good results on other problems.
% 所以我们没有两阶段，先训练再应用的方式，而是直接使用端到端的在线控制，利用当前信息训练模型，预测之后的选择算子。
so we don't adopt the two-stage method (train stage and control stage), but adopt the end-to-end online control method, use the current information to train neural network and the use the previous training model to select the operators.

% \subsection{differential evolution}
%%% 先介绍下传统的差分进化，在moead框架下，使用tche分解
% 传统的DE的算子使用de/rand/1和二项式交叉
% The traditional differential evolution algorithm \cite{storn1997differential} uses \textit{DE/rand/1} as the mutation strategy and in order to increase the diversity of vectors, binomial crossover is employed.
\subsection{MOEA/D}
%%%%  介绍MOEAD，介绍MOEAD-DRA，介绍DE的变体，说明算子池，
% 为了将AOS应用到多目标问题上，我们需要一个多目标的框架，moea/d将多目标问题分解成单目标问题，可以
% 大多数的AOS方法都是单目标的，比如DE_DQN and LDE,因为在 Credit Assignment阶段需要评价算子的效果，但是在多目标问题中很难评价两个解的好坏
Most of the AOS methods are applied to single-objective problem, such as DE-DQN\cite{sharma2019deep} and LDE \cite{sun2021learning}, because the performance of operators should be estimated at the credit assignment stage, but it is difficult to define the quality difference between two non-dominated solutions and the improvement brought by the application of an operator.
% 在基于支配的多目标框架中，比如NSGAII，SPEA2，随着目标数增加，非支配解的数量也会快速增加。
In some algorithmic frameworks which based on dominance relation, such as NSGA-II\cite{nsga2}, SPEA2 \cite{spea2}, with the increase of the number of objectives, the number of non-dominated solutions will increase rapidly,
% 如果一个父代生成了非支配的子代，我们很难衡量本次算子的效果
if a parent generates a non-dominated offspring, it is difficult to measure the effect of this operator.

% 这篇工作中我们使用基于分解的多目标优化算法，MOEAD，将多目标问题分解成多个单目标子问题，利用分解的策略，单目标的credit assignment技术可以应用到moead 
In this paper, we applied the multi-objective evolutionary framework based on decomposition (MOEA/D) \cite{moead} which decompositions a multi-objective optimization problem into plenty of scalar optimization subproblem. Benefit from the decomposition, the credit assignment methods in single-objective can be applied to the MOEA/D framework.
% 在mop中，一个Pareto最优解就是在某个问题上达到了最优解，而在所有问题上的加权集合构成了Pareto集合，
In multi-objective problems, a Pareto optimal solution is to reach the optimal solution on a single-objective optimization problem, and the weighted aggregation on all objectives constitute the Pareto set.
% 为了得到Pareto set， moead利用权重向量将不同的目标组合成多个单目标子问题，在单目标上的最优解构成了MOP的Pareto set。
To obtain the Pareto set, MOEA/D uses weight vector to combine different objectives into multiple single-objective subproblems, and the optimal solutions on the single-objective constitutes the optimal solutions of MOP.
% 由于不同子问题间差异过大，moead将子问题按照权重向量分组，使用相邻的解优化当前子问题。
Due to the great difference between different subproblems, MOEA/D groups the subproblems according to the weight vector and optimizes the current subproblems with neighboring subproblemss.
% moead的tech分解法
Three approaches are introduced in MOEA/D to transform the approximation problem of PF into multiple scalar optimization problems \cite{moead}, in this paper, we employ the Tchebycheff approache, the scalar optimization problem is in the form
\begin{equation}
  \begin{aligned}
     & \text { minimize } g^{\text {te }}\left(x \lambda, z^{*}\right)=\max _{1 \leq i \leq m}\left\{\lambda_{i} f_{i}(x)-z_{i}^{*}\right\} \\
     & \text { subject to } x \in \Omega
  \end{aligned}
\end{equation}
% 介绍公式里的参数 z，lambda， 
where $z^{*}=\left(z_{1}^{*}, \ldots, z_{m}^{*}\right)^{T}$ is the idea point, i.e. $z_{i}^{*}=\min \left\{f_{i}(x) \mid x \in \Omega\right\}$ for each $i = 1, \dots , m$. $\Lambda=\left\{\lambda^{1}, \ldots, \lambda^{N}\right\}$ is the set of weight vector satisfies $\sum{^m_{i=1} \lambda ^j_i=1}$ and $\lambda^j_i \geq 0$ for all $i \in \{1,\dots, m\}$.

% 在moead中，所有的子问题会被同等的对待，并且被分配了同等的计算量
In the original MOEA/D, all subproblems are treated equally and get the same computing resources,
% 但这些子问题有不同的难度，因此在这篇论文里我们用moead-dra。
but these subproblems may have different complexity, therefore, in this paper, we uses MOEA/D with dynamical resource allocation (MOEA/D-DRA) \cite{moead-dra} which won the CEC09 Uncomstrained MOP test.
% 对每个子问题，定义一个pi衡量子问题的重要程度
For each subproblem $i$, $\pi^i$ was defined to measure the improvement of $x^i$ in reducing the objective of this subproblem.

During the search, MOEA/D-DRA works as follows:
% moead-dra的工作流程
\begin{enumerate}
  % 计算权重向量之间的欧氏距离，记录每个向量最近的T个向量
  \item The Euclidean distance between weight vectors is calculated, and the index of the nearest $t$ weight vectors of each vector is recorded, $T$-neighborhoods of $x_i$ are $B(i) = \{i_i,\dots , i_T\}$ where $\lambda^{i_1},\dots,\lambda^{i_T}$ are the $T$ closest weight vectors to $\lambda^i$.
        % 使用pi衡量xi在减少子问题上的效用。
  \item $\pi^i$ is defined to measure the effect of $x^i$ on the optimization subproblem, the indicator is defined as
        \begin{equation}
          \pi^{i}=\left\{\begin{array}{ll}
            1,                                                                     & \text { if } \Delta^{i}>0.001 \\
            \left(0.95+0.05 \times \frac{\Delta^{i}}{0.001}\right) \times \pi^{i}, & \text { otherwise }
          \end{array}\right.
          \label{eq: delta}
        \end{equation}
        % 根据pi使用10-tournament selection选择N/5-m个索引加入I
        By using 10-tournament selection based on $\pi$, select $\frac{N}{5}-m$ indexes other than $i$ and add them to set $I$.
        % 随机生成rand，确定父代范围
  \item Uniformly randomly generate a number \textit{rand} to determine parent range
        \begin{equation}
          P=\left\{\begin{array}{ll}
            B(i)             & \text { if } \text { rand }<\delta \\
            \{1, \ldots, N\} & \text { otherwise }
          \end{array}\right.
        \end{equation}
        % 从P中选择两个index，r2和r3作为父代，利用当前解i作为r1,根据DE算子生成子代y,根据y更新z
  \item Randomly select two indexes $r_2$, $r_3$ from $P$ and set $r_1=i$, apply DE operator on the above selected indexes to generate the offspring $y$ and update the best value $z^*$ found so far, for each $i=1,\dots,m$, if $z_i^* > f_i(y)$, then set $z_i=f_i(y)$.
        % 随机从P中选择一个index j，如果生成的解y比j好，就替换j，重复直到P为空或是替换个体到达上限nr
  \item Replace at most $n_r$ solutions in $P$ by $y$ if $g\left(y \mid \lambda^{j}, z\right) \leq g\left(x^{j} \mid \lambda^{j}, z\right), x \in P$ and then set $x^j=y$.
        % 每50代更新一次delta^i，在过去50代中子问题目标的相对减少
  \item The indicator $\pi^i$ was updated every 50 generations according to the formula \ref{eq: delta} and compute the relative decrease $\delta^i$ of the objective for each subproblem $i$ during the last 50 generations.
\end{enumerate}





\begin{equation}
  v^{i}=x^{i}+F \times\left(x^{r_{1}}-x^{r_{2}}\right)
\end{equation}
Where F

\subsection{Markov decision process}
%%% 建模成MDP

\subsection{moead-rl}
%%% 将强化学习嵌入到DE的框架

\subsection{Double DQN}

\subsection{Computational Complexity of MOEA/PSL}


\section{Empirical Studies}


\subsection{Comparative Algorithms}

\subsection{Test Problems}

\subsection{Results on Benchmark Problems}

\subsection{Results on Real-World Problems}

\subsection{Effectiveness of the Two Neural Networks in MOEA/PSL}\label{sec:twoNNs}

\subsection{Effectiveness of the Parameter Adaptation Strategy in MOEA/PSL}

\section{Conclusions}


 (e.g., deep neural network training)

\ifCLASSOPTIONcaptionsoff
  \newpage
\fi

\bibliographystyle{IEEEtran}
\bibliography{references}

% \iffalse
%   \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{XingyiZhang.eps}}]{Xingyi Zhang}
%     received the B.Sc. from Fuyang Normal College in 2003, and the M.Sc. in 2006 and Ph.D. in 2009
%     both from Huazhong University of Science and Technology. Currently, he is
%     an associate professor in the School of Computer Science and Technology, Anhui University. His main research interests
%     include unconventional models and algorithms of computation, multi-objective optimization and membrane computing.
%   \end{IEEEbiography}
% \fi

\end{document}